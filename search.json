[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "flyswot-gym",
    "section": "",
    "text": "flyswot-gym is a set of functions for helping to train flyswot models. This is a WIP and mainly intended for our particular workflow."
  },
  {
    "objectID": "index.html#trainingupdating-a-new-flyswot-model",
    "href": "index.html#trainingupdating-a-new-flyswot-model",
    "title": "flyswot-gym",
    "section": "ðŸš€ Training/updating a new flyswot model",
    "text": "ðŸš€ Training/updating a new flyswot model\nThese steps will also work for training other image classification models thought the pipeline isnâ€™t intended to cover all possible scenarios to you may not get good results if you are doing something very different.\n\nLoading our data\nThe data loading workflow described below assumes that you are working with data structured in an image folder structure. In this structure images are sorted into subdirectories with those directories representing the label for that image. For example:\nImages/\n    dogs/\n        dog1.jpg\n        dog2.jpg\n    cats/\n        cats1.jpg\n        cats2.jpg\nWe use the Hugging Face hub to store the data we are going to use for training. To upload our data from a local file system to the hub, see the docs page on preparing data for flyswot."
  },
  {
    "objectID": "index.html#trainingupdating-a-model",
    "href": "index.html#trainingupdating-a-model",
    "title": "flyswot-gym",
    "section": "Training/updating a model",
    "text": "Training/updating a model\nOnce you have your data uploaded to the ðŸ¤— hub we can train our or update a new model. There is a notebok which helps us do this ðŸ¦¾"
  },
  {
    "objectID": "index.html#using-our-model",
    "href": "index.html#using-our-model",
    "title": "flyswot-gym",
    "section": "Using our model",
    "text": "Using our model\nFor using a new model you can use flyswot. You can pass in the model ID for your model to let flyswot know which model it should use for making predictions.\nUsage: flyswot predict directory [OPTIONS] DIRECTORY CSV_SAVE_DIR\n\n  Predicts against all images stored under DIRECTORY which match PATTERN in the\n  filename.\n\n  By default searches for filenames containing 'fs'.\n\n  Creates a CSV report saved to `csv_save_dir`\n\nArguments:\n  DIRECTORY     Directory to start searching for images from  [required]\n  CSV_SAVE_DIR  Directory used to store the csv report  [required]\n\nOptions:\n  --model-id TEXT       The model flyswot should use for making predictions\n                        [default: flyswot/convnext-tiny-224_flyswot]\n  --pattern TEXT        Pattern used to filter image filenames\n  --bs INTEGER          Batch Size  [default: 16]\n  --image-formats TEXT  Image format(s) to check  [default: .tif]\n  --help                Show this message and exit."
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Core functionality",
    "section": "",
    "text": "#hide\nfrom nbdev.showdoc import *\ntesting"
  },
  {
    "objectID": "core.html#data-loading",
    "href": "core.html#data-loading",
    "title": "Core functionality",
    "section": "Data loading",
    "text": "Data loading\n\n# !git clone https://huggingface.co/datasets/davanstrien/testgitupload\n\n\ntestdataset = Path(\"testdataset\")\ntestdataset.mkdir()\n\n\ntest_images = {\"control\": Image.open(\"testdata/add_ms_05422_fcontrol1.jpg\"),\n\"flysheet\": Image.open(\"testdata/add_ms_9403_fse001v.jpg\")}\n\n\nfor label in {\"control\",\"flysheet\"}:\n    folder = testdataset/label\n    folder.mkdir()\n    image = test_images[label]\n    for i in range(100):\n        fname = f\"{i}_{label}\"\n        image.save(f\"{folder}/{fname}.jpg\")\n\n\nds = load_dataset(\"imagefolder\",data_dir=\"testdataset\",\n                  streaming=False,\n                  split='train')\n\n\n\n\nUsing custom data configuration default-2cddcb8c7d26937e\n\n\nDownloading and preparing dataset imagefolder/default to /Users/dvanstrien/.cache/huggingface/datasets/imagefolder/default-2cddcb8c7d26937e/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f...\n                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset imagefolder downloaded and prepared to /Users/dvanstrien/.cache/huggingface/datasets/imagefolder/default-2cddcb8c7d26937e/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f. Subsequent calls will reuse this data.\n\n\n\nds\n\nDataset({\n    features: ['image', 'label'],\n    num_rows: 200\n})\n\n\n\nds[0]['image'].filename\n\n'/Users/dvanstrien/Documents/DS/gym/nbs/testdataset/control/0_control.jpg'\n\n\n\nsource\n\nfilter_bad_images\n\n filter_bad_images (ds:datasets.arrow_dataset.Dataset)\n\n\nds = filter_bad_images(ds)\n\n\n\n\n\nds\n\nDataset({\n    features: ['image', 'label'],\n    num_rows: 200\n})\n\n\n\nsource\n\n\nget_fpath\n\n get_fpath (ds:datasets.arrow_dataset.Dataset)\n\n\nds = get_fpath(ds)\n\n\n\n\n\nds[0]\n\n{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=687x1000>,\n 'label': 0,\n 'fpath': '/Users/dvanstrien/Documents/DS/gym/nbs/testdataset/control/0_control.jpg'}\n\n\n\nf =  '/Users/dvanstrien/Documents/DS/hmd_flysheet_detection/data/Flysheet_data/CONTAINER/or_5268_fse002r/Users/dvanstrien/Documents/DS/hmd_flysheet_detection/data/Flysheet_data/CONTAINER/or_5268_fse002r (1).jpg.jpg'\n\n\nf\n\n'/Users/dvanstrien/Documents/DS/hmd_flysheet_detection/data/Flysheet_data/CONTAINER/or_5268_fse002r/Users/dvanstrien/Documents/DS/hmd_flysheet_detection/data/Flysheet_data/CONTAINER/or_5268_fse002r (1).jpg.jpg'\n\n\n\nf = re.sub(r\"(\\(\\d\\))\",\"\",f)\n\n\nf.split('.')[0]\n\n'/Users/dvanstrien/Documents/DS/hmd_flysheet_detection/data/Flysheet_data/CONTAINER/or_5268_fse002r/Users/dvanstrien/Documents/DS/hmd_flysheet_detection/data/Flysheet_data/CONTAINER/or_5268_fse002r '\n\n\n\nsource\n\n\nreturn_base_path_deduplicated\n\n return_base_path_deduplicated (x)\n\n\nsource\n\n\ncheck_uniques\n\n check_uniques (example, uniques, column='clean_path')\n\n\nsource\n\n\ndrop_duplicates\n\n drop_duplicates (ds)\n\n\nds = drop_duplicates(ds)\n\n\n\n\n\n\n\n\nds\n\nDataset({\n    features: ['image', 'label', 'fpath', 'clean_path'],\n    num_rows: 200\n})\n\n\n\nsource\n\n\nget_id\n\n get_id (example)"
  },
  {
    "objectID": "core.html#train-valid-test-splits",
    "href": "core.html#train-valid-test-splits",
    "title": "Core functionality",
    "section": "Train, valid, test splits",
    "text": "Train, valid, test splits\n\nsource\n\nsplit_w_stratify\n\n split_w_stratify (ds, test_size:Union[int,float],\n                   random_state:Union[int,numpy.random.mtrand.RandomState,\n                   NoneType]=None)\n\n\ntrain, valid = split_w_stratify(ds, test_size=0.5)\n\ntest frequencies\n\nassert_allclose(train.shape, valid.shape,rtol=2)\n\n\ntrain_freqs = frequencies(train['label'])\ntrain_freqs\n\n{1: 50, 0: 50}\n\n\n\ntrain_percentages =  OrderedDict(sorted(valmap(lambda x: x/len(train_freqs),train_freqs).items())).values()\ntrain_percentages\n\nodict_values([25.0, 25.0])\n\n\n\nvalid_freqs = frequencies(valid['label'])\nvalid_percentages = OrderedDict(sorted(valmap(lambda x: x/len(valid_freqs),valid_freqs).items())).values()\nvalid_percentages\n\nodict_values([25.0, 25.0])\n\n\n\nassert_allclose(list(train_percentages), list(valid_percentages), atol=1)\n\n\nsource\n\n\ntrain_valid_split_w_stratify\n\n train_valid_split_w_stratify (ds, valid_size:Union[int,float]=None,\n                               test_size:Union[int,float]=0.2,\n                               train_size:Union[int,float,NoneType]=None, \n                               random_state:Union[int,numpy.random.mtrand.\n                               RandomState,NoneType]=None)\n\n\ntrain, valid, test = train_valid_split_w_stratify(ds)\n\n\nsource\n\n\nprepare_dataset\n\n prepare_dataset (ds)\n\n\nds = load_dataset(\"imagefolder\",data_dir=\"testdataset\",\n                  split='train',use_auth_token=True)\n\n\n\n\nUsing custom data configuration default-2cddcb8c7d26937e\nFound cached dataset imagefolder (/Users/dvanstrien/.cache/huggingface/datasets/imagefolder/default-2cddcb8c7d26937e/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n\n\n\ntrain,valid,test = prepare_dataset(ds)\ntrain,valid,test\n\nPreparing dataset...\n\n\n\ndropping duplicates...\n\n\n\n\n\n\ncreating train, valid, test splits...\n\n\n\ntrain has 160 examples\n\n\n\nvalid has 36 examples\n\n\n\ntest has 4 examples\n\n\n\n(Dataset({\n     features: ['image', 'label'],\n     num_rows: 160\n }),\n Dataset({\n     features: ['image', 'label'],\n     num_rows: 36\n }),\n Dataset({\n     features: ['image', 'label'],\n     num_rows: 4\n }))"
  },
  {
    "objectID": "core.html#augmentations",
    "href": "core.html#augmentations",
    "title": "Core functionality",
    "section": "Augmentations",
    "text": "Augmentations\n\nmodel_checkpoint = \"davanstrien/vit-base-patch16-224-in21k-base-manuscripts\"\n\n\nsource\n\nprepare_transforms\n\n prepare_transforms (model_checkpoint, train_ds, valid_ds, test_ds=None)\n\n\ntrain_ds, valid_ds, test_ds = prepare_transforms(model_checkpoint, train,valid, test)\n\n\nsource\n\n\nFlyswotData\n\n FlyswotData (train_ds:datasets.arrow_dataset.Dataset,\n              valid_ds:datasets.arrow_dataset.Dataset,\n              test_ds:datasets.arrow_dataset.Dataset,\n              id2label:Dict[int,str], label2id:Dict[str,int])\n\n\nsource\n\n\nprep_data\n\n prep_data (ds, model_checkpoint=None)\n\n\ndata = prep_data(ds, model_checkpoint=model_checkpoint)\n\nPreparing dataset...\n\n\n\ndropping duplicates...\n\n\n\n\n\n\ncreating train, valid, test splits...\n\n\n\ntrain has 795 examples\n\n\n\nvalid has 179 examples\n\n\n\ntest has 20 examples\n\n\n\nFlyswotData(train_ds=Dataset({\n    features: ['image', 'label'],\n    num_rows: 795\n}), valid_ds=Dataset({\n    features: ['image', 'label'],\n    num_rows: 179\n}), test_ds=Dataset({\n    features: ['image', 'label'],\n    num_rows: 20\n}), id2label={0: 'CONTAINER', 1: 'CONTROL%20SHOT', 2: 'COVER', 3: 'EDGE%20%2B%20SPINE', 4: 'FLYSHEET', 5: 'OTHER', 6: 'PAGE%20%2B%20FOLIO', 7: 'SCROLL'}, label2id={'CONTAINER': 0, 'CONTROL%20SHOT': 1, 'COVER': 2, 'EDGE%20%2B%20SPINE': 3, 'FLYSHEET': 4, 'OTHER': 5, 'PAGE%20%2B%20FOLIO': 6, 'SCROLL': 7})\n\n\n\ndata\n\nFlyswotData(train_ds=Dataset({\n    features: ['image', 'label'],\n    num_rows: 795\n}), valid_ds=Dataset({\n    features: ['image', 'label'],\n    num_rows: 179\n}), test_ds=Dataset({\n    features: ['image', 'label'],\n    num_rows: 20\n}), id2label={0: 'CONTAINER', 1: 'CONTROL%20SHOT', 2: 'COVER', 3: 'EDGE%20%2B%20SPINE', 4: 'FLYSHEET', 5: 'OTHER', 6: 'PAGE%20%2B%20FOLIO', 7: 'SCROLL'}, label2id={'CONTAINER': 0, 'CONTROL%20SHOT': 1, 'COVER': 2, 'EDGE%20%2B%20SPINE': 3, 'FLYSHEET': 4, 'OTHER': 5, 'PAGE%20%2B%20FOLIO': 6, 'SCROLL': 7})\n\n\n\nfrom dataclasses import asdict\n\n\ntrain_ds, valid_ds, test_ds, id2label, label2id = asdict(data).values()\n\n\ntrain_ds\n\nDataset({\n    features: ['image', 'label'],\n    num_rows: 795\n})"
  },
  {
    "objectID": "core.html#model-training",
    "href": "core.html#model-training",
    "title": "Core functionality",
    "section": "Model training",
    "text": "Model training\n\nsource\n\ncollate_fn\n\n collate_fn (examples)\n\n\nsource\n\n\ntrain_model\n\n train_model (data, model_checkpoint, num_epochs=50, hub_model_id=None,\n              tune=False, fp16=True)\n\n\ntrainer = train_model(data, \"facebook/deit-tiny-patch16-224\",0.001, fp16=False, hub_model_id='test')\n\nDataset({\n    features: ['image', 'label'],\n    num_rows: 795\n})\n\n\n\nSome weights of ViTForImageClassification were not initialized from the model checkpoint at facebook/deit-tiny-patch16-224 and are newly initialized because the shapes did not match:\n- classifier.weight: found shape torch.Size([1000, 192]) in the checkpoint and torch.Size([8, 192]) in the model instantiated\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([8]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n***** Running training *****\n  Num examples = 795\n  Num Epochs = 1\n  Instantaneous batch size per device = 4\n  Total train batch size (w. parallel, distributed & accumulation) = 4\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1\n  Number of trainable parameters = 5525960\n\n\n\n\n    \n      \n      \n      [2/1 : < :, Epoch 0.01/1]\n    \n    \n  \n \n      Epoch\n      Training Loss\n      Validation Loss\n    \n  \n  \n  \n\n\n\n***** Running Evaluation *****\n  Num examples = 179\n  Batch size = 4\n/Users/dvanstrien/Documents/DS/gym/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nSaving model checkpoint to output_dir/checkpoint-1\nConfiguration saved in output_dir/checkpoint-1/config.json\nModel weights saved in output_dir/checkpoint-1/pytorch_model.bin\nImage processor saved in output_dir/checkpoint-1/preprocessor_config.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from output_dir/checkpoint-1 (score: 0.20914715701875866)."
  },
  {
    "objectID": "core.html#model-management",
    "href": "core.html#model-management",
    "title": "Core functionality",
    "section": "Model management",
    "text": "Model management\n\ndata\n\nFlyswotData(train_ds=Dataset({\n    features: ['image', 'label'],\n    num_rows: 795\n}), valid_ds=Dataset({\n    features: ['image', 'label'],\n    num_rows: 179\n}), test_ds=Dataset({\n    features: ['image', 'label'],\n    num_rows: 20\n}), id2label={0: 'CONTAINER', 1: 'CONTROL%20SHOT', 2: 'COVER', 3: 'EDGE%20%2B%20SPINE', 4: 'FLYSHEET', 5: 'OTHER', 6: 'PAGE%20%2B%20FOLIO', 7: 'SCROLL'}, label2id={'CONTAINER': 0, 'CONTROL%20SHOT': 1, 'COVER': 2, 'EDGE%20%2B%20SPINE': 3, 'FLYSHEET': 4, 'OTHER': 5, 'PAGE%20%2B%20FOLIO': 6, 'SCROLL': 7})"
  },
  {
    "objectID": "core.html#model-evaluation",
    "href": "core.html#model-evaluation",
    "title": "Core functionality",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\noutputs = trainer.predict(data.test_ds)\n\n***** Running Prediction *****\n  Num examples = 20\n  Batch size = 4\n\n\n\n    \n      \n      \n      [1/5 : < :]\n    \n    \n\n\n/Users/dvanstrien/Documents/DS/gym/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\noutputs.metrics\n\n{'test_loss': 1.9036948680877686,\n 'test_precision': 0.20238095238095236,\n 'test_recall': 0.16666666666666666,\n 'test_f1': 0.18095238095238095,\n 'test_accuracy': 0.3,\n 'test_runtime': 12.3261,\n 'test_samples_per_second': 1.623,\n 'test_steps_per_second': 0.406}\n\n\n\ny_true = outputs.label_ids\ny_true\n\narray([3, 6, 4, 6, 6, 2, 4, 4, 3, 4, 6, 1, 6, 0, 4, 4, 5, 2, 2, 6])\n\n\n\ny_pred = outputs.predictions.argmax(1)\ny_pred\n\narray([2, 6, 5, 6, 6, 2, 6, 5, 4, 4, 5, 5, 5, 5, 3, 5, 6, 4, 4, 6])\n\n\n\n\n\ndict_values(['CONTAINER', 'CONTROL%20SHOT', 'COVER', 'EDGE%20%2B%20SPINE', 'FLYSHEET', 'OTHER', 'PAGE%20%2B%20FOLIO', 'SCROLL'])\n\n\n\nsource\n\nplot_confusion_matrix\n\n plot_confusion_matrix (outputs, trainer)\n\n\nplot_confusion_matrix(outputs,trainer)\n\nValueError: The number of FixedLocator locations (7), usually from a call to set_ticks, does not match the number of ticklabels (8).\n\n\n\n\n\n\nsource\n\n\ncreate_classification_report\n\n create_classification_report (outputs, trainer)\n\n\nresults = create_classification_report(outputs, trainer)\n\nValueError: Number of classes, 7, does not match size of target_names, 8. Try specifying the labels parameter\n\n\n\nresults\n\n\nmetrics = trainer.evaluate()\ntrainer.log_metrics(\"eval\", metrics)\ntrainer.save_metrics(\"eval\", metrics)\n\n\n# kwargs = {\n#         \"tasks\": \"image-classification\",\n#         \"tags\": [\"image-classification\", \"vision\"],\n#     }\n   \n# #trainer.push_to_hub(**kwargs)\n\n\n# misclasified report\n\n\ndata.test_ds[0]['image']\n\n\nsource\n\n\ncreate_test_results_df\n\n create_test_results_df (outputs, trainer, important_label=None,\n                         print_results=True, return_df=False)\n\n\ndf = create_test_results_df(outputs, trainer, print_results=True,return_df=True)\n\n\n# #export\n# @pn.depends(index_selection)\n# def get_image(selection):\n#     id2label = trainer.config.id2label\n#     image = flyswot_data.test_ds[selection]['image']\n#     image = pn.Pane(image)\n#     row = flyswot_data.test_ds[selection]\n#     string_label = id2label[row['label']]\n#    # label =  pn.pane.Markdown(f\"\"\"actual label: **{string_label}**\"\"\")\n#     df_row = df.iloc[selection]\n#     r = pn.Row(image, pn.Pane(df_row))\n#     return r\n\n\nsource\n\n\ncreate_mistakes_image_navigator\n\n create_mistakes_image_navigator (test_results_df, flyswot_data, trainer)\n\n\n# explorer = create_mistakes_image_navigator(df, data,trainer)\n\n\n# explorer\n\n\n# assert explorer\n\n\n# index_selection = pn.widgets.DiscreteSlider(options=df.index.to_list())\n\n\n# df = df.reset_index(drop=True)\n\n\nsource\n\n\ncreate_misclassified_report\n\n create_misclassified_report (outputs, trainer, test_data,\n                              important_label=None, print_results=True,\n                              return_df=False)\n\n\nassert isinstance(df, pd.DataFrame)\n\n\ndf.y_prob.max()\n\n\ndf[df['y_pred']=='FLYSHEET']\n\n\ndf[df['y_pred']=='FLYSHEET'].sort_values('y_prob',ascending=False)\n\n\n#trainer.push_to_hub()\n\n\n#import pandas as pd\n\n\n# y_true = outputs.label_ids\n# y_pred = outputs.predictions.argmax(1)\n# df = pd.DataFrame({\"y_true\":y_true,\"y_pred\": y_pred})\n\n\n#tidy\n\n\nshutil.rmtree(\"output_dir\")\nshutil.rmtree(\"testdataset\")"
  },
  {
    "objectID": "loading_data.html",
    "href": "loading_data.html",
    "title": "Preparing data for flyswot gym",
    "section": "",
    "text": "To use flyswot-gym we need to ensure that we have data available in a form that flyswot-gym can work with. Specifically flyswot-gym expects data to be available on the Hugging Face Hub. This document explains one way of getting data into the hub (in the right format)."
  },
  {
    "objectID": "loading_data.html#what-you-need-to-have-installed",
    "href": "loading_data.html#what-you-need-to-have-installed",
    "title": "Preparing data for flyswot gym",
    "section": "What you need to have installed",
    "text": "What you need to have installed\nThere are a few things you will need to have installed before you can follow the remaining steps.\n\nGit\ngit is an open-source version control system. We use this along with git-lfs (see below) to upload our data to the hub.\n\n\ngit-lfs\ngit is primarily intended for versioning software source code. This means git is primarily intended for working with relatively small plain text files.\ngit-lfs (Git Large File Storage (LFS) ) is an extension for git which adds support for versioning large files. In our case this will mean that we can version our image files more easily.\n\n\n\n\n\n\nInstalling git and git-lfs\n\n\n\nIf you are running Linux you should be able to install git and git-lfs via your package manager. If you are running macOS you should be able to install via homebrew.\nOn Windows, using https://gitforwindows.org/Â to install git will also install git-lfs."
  },
  {
    "objectID": "loading_data.html#hugging-face-hub",
    "href": "loading_data.html#hugging-face-hub",
    "title": "Preparing data for flyswot gym",
    "section": "Hugging Face hub",
    "text": "Hugging Face hub\nWeâ€™ll also install the huggingface_hub python library. This will allow us to authenticate our Hugging Face account."
  },
  {
    "objectID": "loading_data.html#hugging-face-account",
    "href": "loading_data.html#hugging-face-account",
    "title": "Preparing data for flyswot gym",
    "section": "Hugging Face account",
    "text": "Hugging Face account\nIn oder to use the Hugging Face hub we need need to have an account with Hugging Face. If you donâ€™t already have an account you can create one here: https://huggingface.co/join."
  },
  {
    "objectID": "loading_data.html#installing-the-huggingface_hub-library",
    "href": "loading_data.html#installing-the-huggingface_hub-library",
    "title": "Preparing data for flyswot gym",
    "section": "Installing the huggingface_hub library",
    "text": "Installing the huggingface_hub library\nWe can install the huggingface_hub library using pip:\npip install huggingface_hub"
  },
  {
    "objectID": "loading_data.html#authenticate-with-hugging-face-hub",
    "href": "loading_data.html#authenticate-with-hugging-face-hub",
    "title": "Preparing data for flyswot gym",
    "section": "Authenticate with Hugging Face Hub",
    "text": "Authenticate with Hugging Face Hub\nThere are a few steps we need to take to ensure we can push datasets from our machine to the hugging face hub. Most of these steps will only have to be run one per machine.\n\ngit config\nWe can use the git config command can be used to configure options for git. In this case we set enable a credential helper which will make managing authentication a little easier.\ngit config --global credential.helper store\nFrom here we can use the huggingface-cli login command to ensure we are authenticated with the ðŸ¤— hub.\nhuggingface-cli login"
  },
  {
    "objectID": "loading_data.html#creating-an-imagefolder-dataset",
    "href": "loading_data.html#creating-an-imagefolder-dataset",
    "title": "Preparing data for flyswot gym",
    "section": "Creating an ImageFolder dataset",
    "text": "Creating an ImageFolder dataset\nflyswot-gym expects data to be arranged in an imagefolder style. This format uses folder names to associate images inside that folder with that name. For example:\nImages/\n    dogs/\n        dog1.jpg\n        dog2.jpg\n    cats/\n        cats1.jpg\n        cats2.jpg\nWhilst this format has limitations it doesnâ€™t require special software to create and itâ€™s simple to explain to people how the format works."
  },
  {
    "objectID": "loading_data.html#upload-data-for-the-first-time",
    "href": "loading_data.html#upload-data-for-the-first-time",
    "title": "Preparing data for flyswot gym",
    "section": "Upload data (for the first time)",
    "text": "Upload data (for the first time)\nThe steps below are for uploading a new dataset for the first time. Below you find instructions for updating a dataset.\n\nCreate a repository\nThe first step to upload a new dataset to the Hugging Face hub is to create a repository for the dataset. There are various ways of doing this but weâ€™ll use the web interface on the Hugging Face website (https://huggingface.co/). Clicking on your profile picture on the top right of the page will show a drop down menu where you can click New Dataset\n\nWhen you click New Dataset you will be presented with some options related to your dataset:\n\nYou will have the option of creating a dataset under a personal account or an organization. If you are planning to use the data for training flyswot models you probably want to use the flyswot Hugging Face organization. This will allow other people who are part of this organization to work with this data.\nYou also have the option to choose a name for your dataset. The name of your dataset doesnâ€™t matter too much but it will be helpful for you later if the dataset naming makes it clear what the data is for.\n\n\n\n\n\n\nKeeping data private\n\n\n\nDepending on what kind of data you are working with you may want decide to share it publicly or keep it private. To keep it private make sure you choose the private option when creating the dataset. Itâ€™s possible to change this later if you decide you want to make the data public.\n\n\n\n\nCreate a folder to use for managing Hugging Face Datasets\nThis step is optional, but you may find it helpful to create a folder/directory to use for managing and storing datasets that are going to be uploaded to the Hugging Face hub. For example you may want to create a folder on your Desktop called huggingface_datasets. This folder can then act as a central place to store datasets you are working with on the Hugging Face hub.\n\n\nClone repo\nOnce you have created a dataset on the Hugging Face hub we can clone it to our machine. When using git the clone command is used to make a copy of a remote dataset (in this case a dataset on the Hub) to another place (in this case to the machine when our data is currently stored). Git-guides has some further information about the clone command if you are interested in learning more.\nTo clone the dataset you need to find the URL for the dataset. To get this you can go to your dataset on the hub and click â€˜Use in dataset libraryâ€™ \nOnce youâ€™ve clicked on this youâ€™ll get a link for clone the dataset\n\nIn your command prompt/terminal navigate to the folder/directory you want to use for storing your dataset.\nFrom here you can run\ngit lfs install\nto install git lfs, followed by\ngit clone URL\nFor example to clone a repo called flyswot inside the flyswot organization you would run:\ngit clone https://huggingface.co/datasets/flyswot/test\n\n\n\n\n\n\nTip\n\n\n\nIf you are running Windows and have git bash installed you can right click on a folder and click git bash here to open the terminal inside that folder. \n\n\n\n\nCopy data\nOnce you have cloned the repo you will see a new folder in the location where you ran the clone command. This folder will have the same name as the repository you just created.\nCopy the image folder dataset containing your images to the directory you just cloned. You can either do this on the command line or use your file explorer.\n\n\nAdd Files\nOnce you have copied the image folder directory into your cloned repository you need to add the files you want git to track. Using git add tells git that you want it to keep track of a certain file for changes and in our case to push these files upstream to a repository.\nIn our case we want to add image files. We can use a â€˜wildcardâ€™ to add all files with a particular extension.\ngit add \"imagefolder/**/*.jpg\"\nThis will recursively add all the files with the extension .jpg. To account for files having the .jpeg extension we can also add these\ngit add \"imagefolder/**/*.jpeg\"\n\n\n\n\n\n\nTip\n\n\n\nYou may also have images stored in other formats such as .tiff files. Whilst you could add these files in the same way as above you may want to convert these files to to JPEG files to reduce the size of your dataset. This will make moving your dataset around quicker.\n\n\n\n\nCommit\nOnce you have added your files you need to use git commit to tell git you want to commit the files you just added to the history of the git repository. This will allow you to push the files you just added to the Hugging Face hub.\nWhen you commit files you usually include a message which acts as a record of what you were doing in that commit. You can pass this message in using the -m flag.\ngit commit -m \"upload training data\" \n\n\nPush\nOnce you have run git commit you can use git push to push your data to the remote repository.\ngit push\nThis will push the files you committed to the repository on the Hugging Face hub. This step may take some time depending on the size of your data.\nOnce the push command has finished you will see the newly added files on the repository on the Hugging Face hub."
  }
]